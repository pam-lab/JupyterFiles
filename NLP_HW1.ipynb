{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-HW1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pam-lab/Farsi_Space_Fixer/blob/main/NLP_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Steps\n",
        "- [x] Work breakdown\n",
        "- [ ] Processing verbs ->(Pouya)\n",
        "- [ ] POS tagging -> (Pouya)\n",
        "- [ ] Specify syllables -> (Mahdi)\n",
        "- [ ] Name Processing(e.g, Plural nouns) ->(Mahdi)\n",
        "- [ ] Adjective Processing(e.g, ترین) ->(Amir)\n",
        "- [ ] Final Report ->(Mahdi)\n",
        "- [ ] Web API(Extra score)\n",
        "\n",
        "## Related Links\n",
        "*    [آداب نوشتار](https://drive.google.com/file/d/1eeE3sWoe-U-YzhbXXKrMlVCzeL8GNcl8/view)\n",
        "\n",
        "*    [نیم‌فاصله-ویکی‌پدیا](https://fa.wikipedia.org/wiki/%D9%88%DB%8C%DA%A9%DB%8C%E2%80%8C%D9%BE%D8%AF%DB%8C%D8%A7:%D9%86%DB%8C%D9%85%E2%80%8C%D9%81%D8%A7%D8%B5%D9%84%D9%87)\n",
        "\n",
        "*    [قوانین دستور خط](https://virastaran.net/khat/)\n",
        "*    [راهنمای پردازش زبان طبیعی-فرادرس](https://blog.faradars.org/practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text/)\n",
        "\n",
        "## Notes\n",
        "\n",
        "\n",
        "*   Please develop sections completely modular with functions.\n",
        "*   Dont forgot clean code !!!\n",
        "*   Use [Hazm](https://github.com/sobhe/hazm) for processing.\n",
        "\n"
      ],
      "metadata": {
        "id": "kcWPuYKvdNMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verbs"
      ],
      "metadata": {
        "id": "3foLsYnmktjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Khalifehsoltani, S.‎‎ N.‎‎, Cholmaghani, A.‎‎, Vahdani, A.‎‎, & Moallemi, R.‎‎ (2010, April).‎‎ Building a large Persian Verb Collection: A generative approach.‎‎ In The 2nd International Conference on Computer Engineering and Technology.‎ ‪(ICCET)‬, IEEE."
      ],
      "metadata": {
        "id": "85koYdqdqtbi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6SKT2i-HW2R"
      },
      "outputs": [],
      "source": [
        "# Download verbs dataset which is downloaded from \n",
        "# https://www.peykaregan.ir/dataset/%D9%85%D8%AC%D9%85%D9%88%D8%B9%D9%87-%D8%A7%D9%81%D8%B9%D8%A7%D9%84-%D8%AA%D8%B5%D8%B1%DB%8C%D9%81%E2%80%8C%D8%B4%D8%AF%D9%87-%D9%81%D8%A7%D8%B1%D8%B3%DB%8C\n",
        "\n",
        "\n",
        "!gdown --id 1aIDGD3hHjDyWZ5i8vmgtMxRAzSxGFCWY \n",
        "\n",
        "!unzip PVC.zip && cd PVC/Data && unzip '*.zip'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xmltodict\n",
        "import xmltodict"
      ],
      "metadata": {
        "id": "GnH-q36726SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "base_path='/content/PVC/Data/XML/'\n",
        "infinitive=Path(base_path+'infinitive.xml').read_text()\n",
        "#infinitive_english_translation=Path(base_path+'infinitive_english_translation.xml').read_text()\n",
        "#verb=Path(base_path+'verb.xml').read_text()\n",
        "\n",
        "infinitive=xmltodict.parse(infinitive)\n",
        "#infinitive_english_translation=xmltodict.parse(infinitive_english_translation)\n",
        "#verb=xmltodict.parse(verb)"
      ],
      "metadata": {
        "id": "MiLgCBMs_Gsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# مصدرها هم فعل در نظر گرفته می‌شوند چون باید نیم فاصله بینشان بیفتد\n",
        "verbs = []\n",
        "for item in infinitive[\"RECORDS\"][\"RECORD\"]:\n",
        "    verbs.append(item['text'])\n",
        "    verbs.append(item['past_stem'])\n",
        "    verbs.append(item['present_stem'])\n",
        "    verbs.append(item['passive_voice'])\n",
        "    verbs.append(item['past_participle'])"
      ],
      "metadata": {
        "id": "0WLGEtUTmokm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization"
      ],
      "metadata": {
        "id": "bhU0ttYW-EF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hazm"
      ],
      "metadata": {
        "id": "bB16iCp06NgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm\n",
        "import hazm"
      ],
      "metadata": {
        "id": "9Ga9AZwCEL72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/sobhe/hazm/releases/download/v0.5/resources-0.5.zip\n",
        "!unzip resources-0.5.zip -d resources/"
      ],
      "metadata": {
        "id": "iPLMSShOk6ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals\n",
        "normalizer = hazm.Normalizer()\n",
        "print(normalizer.normalize('در هنگام وقوع بلایای طبیعی ،بیش‌ترین خسارت متوجه قشر آسیب پذیر جامعه می شود.'))"
      ],
      "metadata": {
        "id": "A3Am5lhN37Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parsivar"
      ],
      "metadata": {
        "id": "m5U-QgCvk24b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install parsivar\n",
        "import parsivar"
      ],
      "metadata": {
        "id": "HQCYf2jtlpAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myText = 'لاک پشت با هوش از همهٔ آنها زیرکانه تر عمل کرد!'\n",
        "my_normalizer = Normalizer()\n",
        "text_normal = my_normalizer.normalize(myText)\n",
        "print(text_normal)"
      ],
      "metadata": {
        "id": "ZekGs0KXlvKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POS Tagging"
      ],
      "metadata": {
        "id": "sgB4TlFR5-r1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hazm"
      ],
      "metadata": {
        "id": "mU_9PpFM6CtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "ویکی‌پدیای فارسی نام یکی از دانشنامه‌های فارسی‌زبان در اینترنت و یکی از نسخه‌های ویکی‌پدیا، از پروژه‌های بنیاد ویکی‌مدیا است . ویکی‌پدیا پروژهٔ بزرگی است که هدف آن ساخت دانشنامه‌هایی با محتوای آزاد با مشارکت همگان و به همهٔ زبان‌های ممکن است.\n",
        "این دانشنامهٔ فارسی ۱۸ سال پیش (دقیقاً در ۲۸ آذر ۱۳۸۲ / ۱۹ دسامبر ۲۰۰۳) فعالیت خود را آغاز کرد و حجم آن در حدود یک‌سالگی به ۱۰۰۰ مقاله، در دوسالگی (بهمن ۱۳۸۴) -با داشتن رتبهٔ سی‌وهشتم در میان ویکی‌پدیاها- به ۱۰هزار مقاله، و در هفت‌سالگی (شهریور ۱۳۸۹) به ۱۰۰هزار نوشتار رسید . ویکی‌پدیای فارسی هم‌اکنون (۱۰ فروردین ۱۴۰۱ خورشیدی برابر با ۳۰ مارس ۲۰۲۲ میلادی) ۸۹۲٬۶۳۹ نوشتار دارد.[۳]\n",
        "در اسفند ۱۳۹۹ (مارس ۲۰۲۱) ویکی‌پدیای فارسی با ۱٬۱۰۷٬۵۵۰ کاربرِ ثبت‌نام‌کرده، در ردهٔ شانزدهم ویکی‌پدیاها، از لحاظ شمار کاربرانِ فعال در ردهٔ یازدهم و از جهت شمار مدیران (۳۷ مدیر) در ردهٔ هجدهم جای دارد .[۳] ویکی‌پدیای فارسی تا پایان فوریه ۲۰۲۱، دارای ۴۱۰ مقاله خوب و ۱۹۲ مقاله برگزیده است.\n",
        "\"\"\"\n",
        "tagger= hazm.POSTagger(model='resources/postagger.model')\n",
        "tagger.tag(hazm.word_tokenize(text))"
      ],
      "metadata": {
        "id": "3RQZuhex5mw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parsivar"
      ],
      "metadata": {
        "id": "d-tMGMPE6FkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "my_tagger = parsivar.POSTagger(tagging_model=\"stanford\")\n",
        "my_tokenizer = parsivar.Tokenizer()\n",
        "text_tokens = my_tokenizer.tokenize_words(my_normalizer.normalize(text))\n",
        "text_tags = my_tagger.parse(my_tokenizer.tokenize_words(my_normalizer.normalize(text)))\n",
        "print(text_tags)"
      ],
      "metadata": {
        "id": "J5CZKZ_K5qJs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}