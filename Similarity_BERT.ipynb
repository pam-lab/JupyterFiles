{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Similarity_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pam-lab/JupyterFiles/blob/main/Similarity_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and processing"
      ],
      "metadata": {
        "id": "O67Vn__gWwG8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiTZi38tNA-R",
        "outputId": "caeeef48-592d-419f-cc63-004b9b8aea78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'course-nlp-ir-1-text-exploring'...\n",
            "remote: Enumerating objects: 401, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 401 (delta 2), reused 0 (delta 0), pack-reused 397\u001b[K\n",
            "Receiving objects: 100% (401/401), 98.78 MiB | 27.27 MiB/s, done.\n",
            "Resolving deltas: 100% (197/197), done.\n",
            "Checking out files: 100% (245/245), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/language-ml/course-nlp-ir-1-text-exploring"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random \n",
        "import nltk\n",
        "import pandas as pd\n",
        "import codecs\n",
        "import tqdm\n",
        "\n",
        "quranic_dir = \"/content/course-nlp-ir-1-text-exploring/exploring-datasets/religious_text\"\n",
        "df_quran = pd.read_csv(f'{quranic_dir}/quranic_data/id_text_with_orthographies.txt', sep='\\t', header=None)\n",
        "df_nahj = pd.read_csv(f'{quranic_dir}/nahj-al-balaqa/Nahj Al-Balaqa.txt', sep='\\t',header=None)\n",
        "verse_complete_dict = pd.Series(df_quran[1].tolist(), index=df_quran[0]).to_dict()\n",
        "nahj_complete_dict = pd.Series(df_nahj[1].tolist(), index=df_nahj[0]).to_dict()"
      ],
      "metadata": {
        "id": "qqOLyjhGS3cx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "sahife_text=Path(f'{quranic_dir}/Saheefa/sahife_sajjadieh.txt').read_text().split('\\n')\n",
        "sahife_complete_dict = [re.sub('[(][۰-۹]+[)]','', item) for item in sahife_text if item.startswith('(')]"
      ],
      "metadata": {
        "id": "XQ5yonDq5RmL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sahife_complete_dict[12]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zYmWmMuZ6NKq",
        "outputId": "9db2956f-6ae4-4e57-89fc-f2b217c658de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' حَمْداً یَرْتَفِعُ مِنَّا إِلَی أَعْلَی عِلِّیِّینَ فِي کِتَابٍ مَرْقُومٍ یَشْهَدُه\\u200cُ الْمُقَرَّبُونَ. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nahj_complete_dict['2##186']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cCiaD8k30_CF",
        "outputId": "92ae2aa4-3292-4e66-9cb0-842cbe6aea19"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'وَ قَالَ ع لِلظّالِمِ الباَدیِ غَداً بِکَفّهِ عَضّةٌ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verse_complete_dict['2##186']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hG44UkxzTSj_",
        "outputId": "ed748285-24bd-40e0-aac6-573c7c4091e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'وَإِذَا سَأَلَكَ عِبَادِي عَنِّي فَإِنِّي قَرِيبٌ ۖ أُجِيبُ دَعْوَةَ الدَّاعِ إِذَا دَعَانِ ۖ فَلْيَسْتَجِيبُوا لِي وَلْيُؤْمِنُوا بِي لَعَلَّهُمْ يَرْشُدُونَ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize"
      ],
      "metadata": {
        "id": "x0Qok8hCW-MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -Uq camel_tools"
      ],
      "metadata": {
        "id": "dWoFs-Z7VNhh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62bb1c54-575a-4a3d-f5fe-2187f0376316"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 114 kB 8.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 418 kB 64.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 63.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 175 kB 71.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 70.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 53.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.8 MB/s \n",
            "\u001b[?25h  Building wheel for camel-kenlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from camel_tools.utils.normalize import normalize_unicode\n",
        "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
        "from camel_tools.utils.normalize import normalize_alef_ar\n",
        "\n",
        "from camel_tools.utils.normalize import normalize_alef_bw\n",
        "from camel_tools.utils.normalize import normalize_alef_hsb\n",
        "\n",
        "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
        "from camel_tools.utils.dediac import dediac_ar\n",
        "\n",
        "def normalize_arabic(sentence):\n",
        "\n",
        "    # Normalize alef variants to 'ا'\n",
        "    sent_norm = normalize_unicode(sentence)\n",
        "    \n",
        "    sent_norm = normalize_alef_bw(sent_norm)\n",
        "    # Normalize alef variants to 'ا'\n",
        "    sent_norm = normalize_alef_ar(sentence)\n",
        "\n",
        "    # Normalize alef maksura 'ى' to yeh 'ي'\n",
        "    sent_norm = normalize_alef_maksura_ar(sent_norm)\n",
        "\n",
        "    # Normalize teh marbuta 'ة' to heh 'ه'\n",
        "    sent_norm = normalize_teh_marbuta_ar(sent_norm)\n",
        "    return dediac_ar(sent_norm)"
      ],
      "metadata": {
        "id": "SBZgSk_qVfPf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from camel_tools.utils import normalize\n",
        "\n",
        "\n",
        "verse_complete_dict_nrmlz = {k:normalize_arabic(v) for k,v in tqdm.tqdm(verse_complete_dict.items())}\n",
        "nahj_complete_dict_nrmlz = {k:normalize_arabic(v) for k,v in tqdm.tqdm(nahj_complete_dict.items())}\n",
        "sahife_complete_dict_nrmlz = [normalize_arabic(item) for item in tqdm.tqdm(sahife_complete_dict)]"
      ],
      "metadata": {
        "id": "PgTbrIJQV6Do",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91def863-f8a5-4cdb-9e02-280211c3b1b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6236/6236 [00:00<00:00, 16362.77it/s]\n",
            "100%|██████████| 800/800 [00:00<00:00, 3983.14it/s]\n",
            "100%|██████████| 924/924 [00:00<00:00, 9974.20it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(verse_complete_dict['2##186'])\n",
        "print(verse_complete_dict_nrmlz['2##186'])\n",
        "\n",
        "print(nahj_complete_dict['2##186'])\n",
        "print(nahj_complete_dict_nrmlz['2##186'])\n",
        "\n",
        "print(sahife_complete_dict[34])\n",
        "print(sahife_complete_dict_nrmlz[34])"
      ],
      "metadata": {
        "id": "VhfSxrvmWDYr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8072f601-15f2-42be-95bd-85acffd170a5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "وَإِذَا سَأَلَكَ عِبَادِي عَنِّي فَإِنِّي قَرِيبٌ ۖ أُجِيبُ دَعْوَةَ الدَّاعِ إِذَا دَعَانِ ۖ فَلْيَسْتَجِيبُوا لِي وَلْيُؤْمِنُوا بِي لَعَلَّهُمْ يَرْشُدُونَ\n",
            "واذا سالك عبادي عني فاني قريب ۖ اجيب دعوه الداع اذا دعان ۖ فليستجيبوا لي وليؤمنوا بي لعلهم يرشدون\n",
            "وَ قَالَ ع لِلظّالِمِ الباَدیِ غَداً بِکَفّهِ عَضّةٌ\n",
            "و قال ع للظالم البادی غدا بکفه عضه\n",
            " وَ کَاشَفَ فِي الدُّعَاءِ إِلَیْکَ حَامَّتَه‌ُ \n",
            " و کاشف في الدعاء الیک حامته‌ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quranic_tokenized = [sents.split() for sents in tqdm.tqdm(verse_complete_dict_nrmlz.values())]\n",
        "nahj_tokenized = [sents.split() for sents in tqdm.tqdm(nahj_complete_dict_nrmlz.values())]\n",
        "sahife_tokenized = [sents.split() for sents in tqdm.tqdm(sahife_complete_dict_nrmlz)]"
      ],
      "metadata": {
        "id": "MPLCLAWyWvJ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaaf3a7f-d51b-4d2b-e8ff-5f754feec30e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6236/6236 [00:00<00:00, 199069.03it/s]\n",
            "100%|██████████| 800/800 [00:00<00:00, 79745.31it/s]\n",
            "100%|██████████| 924/924 [00:00<00:00, 157044.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "size_quran_token = [len(sentence) for sentence in quranic_tokenized]\n",
        "size_nahj_token =[len(sentence) for sentence in nahj_tokenized]\n",
        "size_sahife_token = [len(sentence) for sentence in sahife_tokenized]\n",
        "fig,axes=plt.subplots(nrows=1,ncols=3,figsize=(10,5))\n",
        "axes[0].hist(size_quran_token)\n",
        "axes[1].hist(size_nahj_token)\n",
        "axes[2].hist(size_sahife_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "PDgW8svJlBc0",
        "outputId": "5d627396-d6e5-4992-8fa0-d3efceaa69eb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([224., 255., 200., 122.,  69.,  32.,  16.,   3.,   2.,   1.]),\n",
              " array([  3. ,  13.7,  24.4,  35.1,  45.8,  56.5,  67.2,  77.9,  88.6,\n",
              "         99.3, 110. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers -Uqq"
      ],
      "metadata": {
        "id": "qMau17rEVOEo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Mini:   asafaya/bert-mini-arabic\n",
        "# Medium: asafaya/bert-medium-arabic\n",
        "# Base:   asafaya/bert-base-arabic\n",
        "# Large:  asafaya/bert-large-arabic\n",
        "# https://www.youtube.com/watch?v=jVPd7lEvjtg\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n",
        "model = AutoModel.from_pretrained(\"asafaya/bert-base-arabic\")\n",
        "\n",
        "model=model.to(device)\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "sm5hDzwI8IKi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b30eb54-5be4-464f-e214-b97b754a178d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PqvM_N8QC2_E"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "\n",
        "batch_values = list(verse_complete_dict_nrmlz.values())\n",
        "\n",
        "encoding=tokenizer.batch_encode_plus(batch_values,\n",
        "                    add_special_tokens=True,\n",
        "                    max_length=128,\n",
        "                    return_token_type_ids=False,\n",
        "                    padding='max_length',\n",
        "                    return_attention_mask=True,\n",
        "                    truncation=True,\n",
        "                    return_tensors='pt'\n",
        "                    )\n",
        "\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "dataset=TensorDataset(encoding.input_ids, encoding.attention_mask)\n",
        "dataloader = DataLoader(dataset,batch_size=128,drop_last=False,shuffle=False)\n",
        "\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for (input_ids, attention_mask) in dataloader:\n",
        "        prediction = model(input_ids.to(device),attention_mask.to(device))\n",
        "        predictions.append(prediction.last_hidden_state)\n",
        "\n",
        "predictions = torch.cat(predictions,dim=0)"
      ],
      "metadata": {
        "id": "S7UcZLFzW2Ty"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask=encoding.attention_mask.unsqueeze(-1).expand(predictions.shape).float()\n",
        "# for filtering out unnecessary ones\n",
        "mask_embeddings=predictions.cpu() * attention_mask\n",
        "\n",
        "summed = torch.sum(mask_embeddings,dim=1)\n",
        "count = torch.clamp(attention_mask.sum(dim=1),min=1e-9)\n",
        "mean_embedding = summed / count\n",
        "mean_embedding.shape\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "COMPARE_FROM = 10\n",
        "mean_embedding = mean_embedding.numpy()\n",
        "similarity=cosine_similarity(\n",
        "    [mean_embedding[COMPARE_FROM]],\n",
        "    mean_embedding \n",
        ").squeeze()\n",
        "\n",
        "most_similar=similarity.argsort()[-10:][::-1]\n",
        "verse_complete=pd.DataFrame(verse_complete_dict_nrmlz.values())\n",
        "print(verse_complete.iloc[COMPARE_FROM].values)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "verse_complete.iloc[most_similar]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "aR06klpMvA0H",
        "outputId": "53ae9d1a-fa88-450f-dc2c-c7f4c4217537"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['والذين يؤمنون بما انزل اليك وما انزل من قبلك وبالاخره هم يوقنون']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                           0\n",
              "10                                                                                           والذين يؤمنون بما انزل اليك وما انزل من قبلك وبالاخره هم يوقنون\n",
              "105                                                                                                  ولقد انزلنا اليك ايات بينات ۖ وما يكفر بها الا الفاسقون\n",
              "3303                                                                                                               الذين اتيناهم الكتاب من قبله هم به يؤمنون\n",
              "1707                                                                          المر ۚ تلك ايات الكتاب ۗ والذي انزل اليك من ربك الحق ولكن اكثر الناس لا يؤمنون\n",
              "476                                                                                       فان كذبوك فقد كذب رسل من قبلك جاءوا بالبينات والزبر والكتاب المنير\n",
              "3735                                                                                                 الم يروا كم اهلكنا قبلهم من القرون انهم اليهم لا يرجعون\n",
              "3684                                                                         وان يكذبوك فقد كذب الذين من قبلهم جاءتهم رسلهم بالبينات وبالزبر وبالكتاب المنير\n",
              "2133                                                                                                وبالحق انزلناه وبالحق نزل ۗ وما ارسلناك الا مبشرا ونذيرا\n",
              "97    واذا قيل لهم امنوا بما انزل الله قالوا نؤمن بما انزل علينا ويكفرون بما وراءه وهو الحق مصدقا لما معهم ۗ قل فلم تقتلون انبياء الله من قبل ان كنتم مؤمنين\n",
              "4122                                                                             ولقد اوحي اليك والي الذين من قبلك لئن اشركت ليحبطن عملك ولتكونن من الخاسرين"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4fda4235-5765-41ec-9543-60bcaf541a7b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>والذين يؤمنون بما انزل اليك وما انزل من قبلك وبالاخره هم يوقنون</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>ولقد انزلنا اليك ايات بينات ۖ وما يكفر بها الا الفاسقون</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3303</th>\n",
              "      <td>الذين اتيناهم الكتاب من قبله هم به يؤمنون</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1707</th>\n",
              "      <td>المر ۚ تلك ايات الكتاب ۗ والذي انزل اليك من ربك الحق ولكن اكثر الناس لا يؤمنون</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>476</th>\n",
              "      <td>فان كذبوك فقد كذب رسل من قبلك جاءوا بالبينات والزبر والكتاب المنير</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3735</th>\n",
              "      <td>الم يروا كم اهلكنا قبلهم من القرون انهم اليهم لا يرجعون</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3684</th>\n",
              "      <td>وان يكذبوك فقد كذب الذين من قبلهم جاءتهم رسلهم بالبينات وبالزبر وبالكتاب المنير</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2133</th>\n",
              "      <td>وبالحق انزلناه وبالحق نزل ۗ وما ارسلناك الا مبشرا ونذيرا</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>واذا قيل لهم امنوا بما انزل الله قالوا نؤمن بما انزل علينا ويكفرون بما وراءه وهو الحق مصدقا لما معهم ۗ قل فلم تقتلون انبياء الله من قبل ان كنتم مؤمنين</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4122</th>\n",
              "      <td>ولقد اوحي اليك والي الذين من قبلك لئن اشركت ليحبطن عملك ولتكونن من الخاسرين</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4fda4235-5765-41ec-9543-60bcaf541a7b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4fda4235-5765-41ec-9543-60bcaf541a7b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4fda4235-5765-41ec-9543-60bcaf541a7b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9bEmLXLe1OzS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}